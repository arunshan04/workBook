{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57784020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql('drop table tablex').collect()\n",
    "spark.sql('create table tablex (a string,id string,part_id string,stime int) stored as parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b460eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('insert into tablex values(\"A\",\"user1\",\"part1\",1),(\"B\",\"user1\",\"part1\",1),(\"C\",\"user1\",\"part2\",1),(\"A\",\"user1\",\"part2\",1),(\"A\",\"user1\",\"part2\",1),(\"A\",\"user1\",\"part26\",1),(\"D\",\"user1\",\"part24\",1),(\"Z\",\"user1\",\"part42\",1),(\"A\",\"user1\",\"part22\",1),(\"A\",\"user12\",\"part2\",1)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3abed1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----+\n",
      "|  a|   id|part_id|stime|\n",
      "+---+-----+-------+-----+\n",
      "|  D|user1| part24|    1|\n",
      "|  Z|user1| part42|    1|\n",
      "|  B|user1|  part1|    1|\n",
      "|  C|user1|  part2|    1|\n",
      "|  D|user1| part24|    1|\n",
      "|  Z|user1| part42|    1|\n",
      "|  A|user1|  part1|    1|\n",
      "|  D|user1| part24|    1|\n",
      "|  Z|user1| part42|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "+---+-----+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.show()\n",
    "df=spark.table('tablex')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbd256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b332781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------------+\n",
      "|part_id|    id|                  ab|\n",
      "+-------+------+--------------------+\n",
      "|  part2| user1|[C, A, A, C, A, A...|\n",
      "| part26| user1|[A, A, A, A, A, A...|\n",
      "|  part2|user12|[A, A, A, A, A, A...|\n",
      "| part22| user1|[A, A, A, A, A, A...|\n",
      "| part24| user1|[D, D, D, D, D, D...|\n",
      "| part42| user1|[Z, Z, Z, Z, Z, Z...|\n",
      "|  part1| user1|[B, A, B, A, B, A...|\n",
      "+-------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=df.groupby('part_id','id').agg(F.collect_list(\"a\").alias('ab') )\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d28d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9cf9e20",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401e3cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(part_id='part2', id='user1', ab='{CC=4194304, AA=16777216, AC=8388608, CA=8388608}'),\n",
       " Row(part_id='part26', id='user1', ab='{AA=4194304}'),\n",
       " Row(part_id='part2', id='user12', ab='{AA=4194304}'),\n",
       " Row(part_id='part22', id='user1', ab='{AA=4194304}'),\n",
       " Row(part_id='part24', id='user1', ab='{DD=4194304}'),\n",
       " Row(part_id='part42', id='user1', ab='{ZZ=4194304}'),\n",
       " Row(part_id='part1', id='user1', ab='{AA=4194304, BB=4194304, AB=4194304, BA=4194304}')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(dir(F))\n",
    "from pyspark.sql.functions import udf\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "def prod(s):\n",
    "    return dict(Counter(list(map(lambda x:x[0]+x[1],list(itertools.product(s,s))))))\n",
    "\n",
    "prod1 = udf(prod)\n",
    "\n",
    "\n",
    "df1.withColumn(\"ab\", prod1(\"ab\")).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63cadd6",
   "metadata": {},
   "source": [
    "# Explain Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "faf72a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [part_id#237, id#236, pythonUDF0#288 AS ab#284]\n",
      "+- BatchEvalPython [prod(ab#261)], [part_id#237, id#236, ab#261, pythonUDF0#288]\n",
      "   +- ObjectHashAggregate(keys=[part_id#237, id#236], functions=[collect_list(a#235, 0, 0)])\n",
      "      +- Exchange hashpartitioning(part_id#237, id#236, 200)\n",
      "         +- ObjectHashAggregate(keys=[part_id#237, id#236], functions=[partial_collect_list(a#235, 0, 0)])\n",
      "            +- *(1) FileScan parquet default.tablex[a#235,id#236,part_id#237] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/arun/Spark/spark-warehouse/tablex], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:string,id:string,part_id:string>\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"ab\", prod1(\"ab\")).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224f934f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4854940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=20480)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('insert into tablex select * from tablex')\n",
    "spark.sql('select count(*) from tablex').collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
