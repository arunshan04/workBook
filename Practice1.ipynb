{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57784020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('drop table tablex').collect()\n",
    "spark.sql('create table tablex (a string,id string,part_id string,stime int) stored as parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b460eeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('insert into tablex values(\"A\",\"user1\",\"part1\",1),(\"B\",\"user1\",\"part1\",1),(\"C\",\"user1\",\"part2\",1),(\"A\",\"user1\",\"part2\",1),(\"A\",\"user1\",\"part2\",1),(\"A\",\"user1\",\"part26\",1),(\"D\",\"user1\",\"part24\",1),(\"Z\",\"user1\",\"part42\",1),(\"A\",\"user1\",\"part22\",1),(\"A\",\"user12\",\"part2\",1)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3abed1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----+\n",
      "|  a|   id|part_id|stime|\n",
      "+---+-----+-------+-----+\n",
      "|  D|user1| part24|    1|\n",
      "|  Z|user1| part42|    1|\n",
      "|  B|user1|  part1|    1|\n",
      "|  C|user1|  part2|    1|\n",
      "|  D|user1| part24|    1|\n",
      "|  Z|user1| part42|    1|\n",
      "|  A|user1|  part1|    1|\n",
      "|  D|user1| part24|    1|\n",
      "|  Z|user1| part42|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "|  A|user1|  part2|    1|\n",
      "+---+-----+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.show()\n",
    "df=spark.table('tablex')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bbd256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b332781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------------+\n",
      "|part_id|    id|                  ab|\n",
      "+-------+------+--------------------+\n",
      "|  part2| user1|[C, A, A, A, A, A...|\n",
      "| part26| user1|[A, A, A, A, A, A...|\n",
      "|  part2|user12|[A, A, A, A, A, A...|\n",
      "| part22| user1|[A, A, A, A, A, A...|\n",
      "| part24| user1|[D, D, D, D, D, D...|\n",
      "| part42| user1|[Z, Z, Z, Z, Z, Z...|\n",
      "|  part1| user1|[A, B, A, B, A, A...|\n",
      "+-------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=df.groupby('part_id','id').agg(F.collect_list(\"a\").alias('ab') )\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d28d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9cf9e20",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "401e3cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| ab|  count|\n",
      "+---+-------+\n",
      "| AA|2097152|\n",
      "| ZZ| 262144|\n",
      "| DD| 262144|\n",
      "| CA| 524288|\n",
      "| CC| 262144|\n",
      "| BA| 262144|\n",
      "| AB| 262144|\n",
      "| AC| 524288|\n",
      "| BB| 262144|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(dir(F))\n",
    "from pyspark.sql.functions import udf\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "def prod(s):\n",
    "    return list(map(lambda x:x[0]+x[1],list(itertools.product(s,s))))\n",
    "\n",
    "prod1 = udf(prod,T.ArrayType(T.StringType()))\n",
    "\n",
    "\n",
    "df2=df1.withColumn(\"ab\", prod1(\"ab\"))\n",
    "df3=df2.withColumn('ab',F.explode('ab'))\n",
    "df3.groupby('ab').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63cadd6",
   "metadata": {},
   "source": [
    "# Explain Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf72a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [part_id#2, id#1, pythonUDF0#133 AS ab#128]\n",
      "+- BatchEvalPython [prod(ab#106)], [part_id#2, id#1, ab#106, pythonUDF0#133]\n",
      "   +- Generate explode(ab#101), [part_id#2, id#1], false, [ab#106]\n",
      "      +- *(2) Project [part_id#2, id#1, pythonUDF0#132 AS ab#101]\n",
      "         +- BatchEvalPython [prod(ab#84)], [part_id#2, id#1, ab#84, pythonUDF0#132]\n",
      "            +- ObjectHashAggregate(keys=[part_id#2, id#1], functions=[collect_list(a#0, 0, 0)])\n",
      "               +- Exchange hashpartitioning(part_id#2, id#1, 200)\n",
      "                  +- ObjectHashAggregate(keys=[part_id#2, id#1], functions=[partial_collect_list(a#0, 0, 0)])\n",
      "                     +- *(1) FileScan parquet default.tablex[a#0,id#1,part_id#2] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/arun/Spark/spark-warehouse/tablex], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:string,id:string,part_id:string>\n"
     ]
    }
   ],
   "source": [
    "df3.withColumn(\"ab\", prod1(\"ab\")).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224f934f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4854940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=5120)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('insert into tablex select * from tablex')\n",
    "spark.sql('select count(*) from tablex').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6592f98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ArrayType', 'AtomicType', 'BinaryType', 'BooleanType', 'ByteType', 'CloudPickleSerializer', 'DataType', 'DataTypeSingleton', 'DateConverter', 'DateType', 'DatetimeConverter', 'DecimalType', 'DoubleType', 'FloatType', 'FractionalType', 'IntegerType', 'IntegralType', 'JavaClass', 'LongType', 'MapType', 'NullType', 'NumericType', 'Row', 'ShortType', 'SparkContext', 'StringType', 'StructField', 'StructType', 'TimestampType', 'UserDefinedType', '_FIXED_DECIMAL', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_acceptable_types', '_all_atomic_types', '_all_complex_types', '_array_signed_int_typecode_ctype_mappings', '_array_type_mappings', '_array_unsigned_int_typecode_ctype_mappings', '_atomic_types', '_check_dataframe_convert_date', '_check_dataframe_localize_timestamps', '_check_series_convert_date', '_check_series_convert_timestamps_internal', '_check_series_convert_timestamps_local_tz', '_check_series_convert_timestamps_localize', '_check_series_convert_timestamps_tz_local', '_check_series_localize_timestamps', '_create_converter', '_create_row', '_create_row_inbound_converter', '_get_local_timezone', '_has_nulltype', '_infer_schema', '_infer_type', '_int_size_to_type', '_make_type_verifier', '_merge_type', '_need_converter', '_parse_datatype_json_string', '_parse_datatype_json_value', '_parse_datatype_string', '_test', '_type_mappings', '_typecode', 'array', 'base64', 'basestring', 'calendar', 'ctypes', 'datetime', 'decimal', 'dt', 'from_arrow_schema', 'from_arrow_type', 'json', 'long', 'platform', 're', 'register_input_converter', 'size', 'sys', 'time', 'to_arrow_schema', 'to_arrow_type', 'unicode']\n"
     ]
    }
   ],
   "source": [
    "print(dir(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57037e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c43c5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf3fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41167670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4c3815f",
   "metadata": {},
   "source": [
    "# Using Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc37510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03764f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.join(df.withColumnRenamed('a','b'),['id','part_id'],'left').withColumn('ab',F.concat(F.col('a'),F.col('b')))\n",
    "df2=df1.select('ab').groupby('ab').agg(F.count('*').alias('Occurance'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c097a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[ab#55], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(ab#55, 200)\n",
      "   +- *(5) HashAggregate(keys=[ab#55], functions=[partial_count(1)])\n",
      "      +- *(5) Project [concat(a#0, b#40) AS ab#55]\n",
      "         +- SortMergeJoin [id#1, part_id#2], [id#46, part_id#47], LeftOuter\n",
      "            :- *(2) Sort [id#1 ASC NULLS FIRST, part_id#2 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(id#1, part_id#2, 200)\n",
      "            :     +- *(1) FileScan parquet default.tablex[a#0,id#1,part_id#2] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/arun/Spark/spark-warehouse/tablex], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:string,id:string,part_id:string>\n",
      "            +- *(4) Sort [id#46 ASC NULLS FIRST, part_id#47 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(id#46, part_id#47, 200)\n",
      "                  +- *(3) Project [a#45 AS b#40, id#46, part_id#47]\n",
      "                     +- *(3) Filter (isnotnull(id#46) && isnotnull(part_id#47))\n",
      "                        +- *(3) FileScan parquet default.tablex[a#45,id#46,part_id#47] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/arun/Spark/spark-warehouse/tablex], PartitionFilters: [], PushedFilters: [IsNotNull(id), IsNotNull(part_id)], ReadSchema: struct<a:string,id:string,part_id:string>\n"
     ]
    }
   ],
   "source": [
    "df2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3580279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| ab|Occurance|\n",
      "+---+---------+\n",
      "| AA|  2097152|\n",
      "| ZZ|   262144|\n",
      "| DD|   262144|\n",
      "| CA|   524288|\n",
      "| CC|   262144|\n",
      "| BA|   262144|\n",
      "| AB|   262144|\n",
      "| AC|   524288|\n",
      "| BB|   262144|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
